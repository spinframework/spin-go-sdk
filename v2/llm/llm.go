package llm

import (
	"fmt"

	"github.com/spinframework/spin-go-sdk/v2/internal/fermyon/spin/v2.0.0/llm"
	"go.bytecodealliance.org/cm"
)

// The model use for inferencing
const (
	Llama2Chat        InferencingModel = "llama2-chat"
	CodellamaInstruct InferencingModel = "codellama-instruct"
)

type InferencingParams struct {
	// The maximum tokens that should be inferred.
	//
	// Note: the backing implementation may return less tokens.
	MaxTokens uint32 `json:"max-tokens"`

	// The amount the model should avoid repeating tokens.
	RepeatPenalty float32 `json:"repeat-penalty"`

	// The number of tokens the model should apply the repeat penalty to.
	RepeatPenaltyLastNTokenCount uint32 `json:"repeat-penalty-last-n-token-count"`

	// The randomness with which the next token is selected.
	Temperature float32 `json:"temperature"`

	// The number of possible next tokens the model will choose from.
	TopK uint32 `json:"top-k"`

	// The probability total of next tokens the model will choose from.
	TopP float32 `json:"top-p"`
}

type InferencingResult struct {
	// The text generated by the model
	// TODO: this should be a stream
	Text string `json:"text"`

	// Usage information about the inferencing request
	Usage InferencingUsage `json:"usage"`
}

// Usage information related to the inferencing result
type InferencingUsage struct {
	_ cm.HostLayout `json:"-"`
	// Number of tokens in the prompt
	PromptTokenCount uint32 `json:"prompt-token-count"`

	// Number of tokens generated by the inferencing operation
	GeneratedTokenCount uint32 `json:"generated-token-count"`
}

// A Large Language Model.
type InferencingModel string

// The model used for generating embeddings
type EmbeddingModel string

type EmbeddingsResult struct {
	// Embeddings are the embeddings generated by the request.
	Embeddings [][]float32
	// Usage is usage related to an embeddings generation request.
	Usage *EmbeddingsUsage
}

type EmbeddingsUsage struct {
	// PromptTokenCount is number of tokens in the prompt.
	PromptTokenCount int
}

// Infer performs inferencing using the provided model and prompt with the
// given optional parameters.
func Infer(model string, prompt string, params *InferencingParams) (InferencingResult, error) {
	iparams := cm.None[llm.InferencingParams]()
	if params != nil {
		iparams = cm.Some(llm.InferencingParams{
			MaxTokens:                    params.MaxTokens,
			RepeatPenalty:                params.RepeatPenalty,
			RepeatPenaltyLastNTokenCount: params.RepeatPenaltyLastNTokenCount,
			Temperature:                  params.Temperature,
			TopK:                         params.TopK,
			TopP:                         params.TopP,
		})
	}

	result := llm.Infer(llm.InferencingModel(model), prompt, iparams)
	if result.IsErr() {
		return InferencingResult{}, errorVariantToError(*result.Err())
	}

	return InferencingResult{
		Text: result.OK().Text,
		Usage: InferencingUsage{
			PromptTokenCount:    result.OK().Usage.PromptTokenCount,
			GeneratedTokenCount: result.OK().Usage.GeneratedTokenCount,
		},
	}, nil
}

// GenerateEmbeddings generates the embeddings for the supplied list of text.
func GenerateEmbeddings(model EmbeddingModel, text []string) (*EmbeddingsResult, error) {
	result := llm.GenerateEmbeddings(llm.EmbeddingModel(model), cm.ToList(text))
	if result.IsErr() {
		return &EmbeddingsResult{}, errorVariantToError(*result.Err())
	}

	embeddings := [][]float32{}
	for _, l := range result.OK().Embeddings.Slice() {
		embeddings = append(embeddings, l.Slice())
	}

	return &EmbeddingsResult{
		Embeddings: embeddings,
		Usage: &EmbeddingsUsage{
			PromptTokenCount: int(result.OK().Usage.PromptTokenCount),
		},
	}, nil
}

func errorVariantToError(err llm.Error) error {
	switch {
	case llm.ErrorModelNotSupported() == err:
		return fmt.Errorf("model not supported")
	case err.RuntimeError() != nil:
		return fmt.Errorf("runtime error %s", *err.RuntimeError())
	case err.InvalidInput() != nil:
		return fmt.Errorf("invalid input %s", *err.InvalidInput())
	default:
		return fmt.Errorf("no error provided by host implementation")
	}
}
